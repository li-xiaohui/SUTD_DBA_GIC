# Investment Insights with NLP
 
 This is a repository for the <i>40.011 : Data and Business Analytics Project</i>, in collaboration with GIC to build a data pipeline that would gather consumer sentiment on various products. 

## Group Members:
1. Matthew Soh
2. Toni Celine Gutierrez Hilos
3. Lam Xue Wei
4. Clarence Toh Jun Wen
5. Ng Au Ker Wesson

## Project Flow

1. Data Collection: Web-scraping of popular e-commerce websites, to build a corpus of product reviews. 
2. Topic Modelling: Building a topic model, using various topic modelling algorithms, trained with the corpus we have built.

## Prerequisites

The script is tested on Python 3.6.5 
Install the following libraries on your system, in order for the code to run. 

1. pandas==0.23.0
2. requests==2.18.4
3. nltk==3.3
4. lxml==4.2.1
5. gensim==3.6.0
6. beautifulsoup4==4.6.3
7. scikit_learn==0.20.0

Alternatively, pip install the requirements.txt
```
pip install -r requirements.txt
```

## Usage

### 1.Data Collection

The main script that triggers the Data Collection process is ```scrape_main.py```. 

```scrape_main.py``` takes in ```SEARCH TERM``` as a keyword, and will initialize the scrapping process from the following websites (more to be added):

1. Amazon (All reviews from the first page of the website's search result)
2. Best Buy (Number of reviews and number of search result pages scraped is up to the user to choose. Default is 40 reviews per product, and the first page of search query)
3. Walmart (Gets the first 20 reviews for each search item, for the first page of the search query, number of pages can be changed.)

<i> *Each scraper is written by a different contributor, thus the functionality is slightly different for each. Furthermore, each scraper addresses the format of a specific website, and is sometimes limited to the restrictions of the website. </i>

The default keyword for ```SEARCH_TERM``` is ```coffee machine```. 
In order to change ```coffee machine``` to whatever you would like to scrape, access the ```scrape_main.py``` file and change the ```SEARCH_TERM```

Scraping process will take ~1 hour, depending on how much there is to scrape.

#### Data Collection: Expected Output:

An excel file labelled <i>'output corpus/Customer Reviews of ```SEARCH_TERM```.xlsx'</i> will be downloaded to your system.

### 2.Topic Modelling

This script will take in the output of ```scrape_main.py```, namely ```'output corpus/Customer Reviews of SEARCH_TERM.xlsx'``` and generate topic models. 
These topic models consists of a certain number of words that probably make up a topic. Currently, we form 2 topic models per algorithm, one consists of positive reviews (where Ratings = 5), while the other consists of negative reviews (where Ratings = 1).

We use three different algorithms to produce the topic models:

1. Latent Dirichlet Allocation (LDA) <i>Blei, David M.; Ng, Andrew Y.; Jordan, Michael I (January 2003)</i>

2. Latent Semantic Analysis (LSA)

3. k-means Clustering

Topic modelling process will take ~15 min, depending on how much data there is to process.

#### Topic Modelling: Expected Output

3 different excel files, labelled <i>'K Means Topic Model.xlsx'</i>, <i>'LDA Topic Model.xlsx'</i>, <i>'LSA Topic Model.xlsx'</i>, will be downloaded to your system. Each excel file contains a number of words generated by the algorithm that most likely form a topic, sorted by brand.

## Future Works

1. As of now, the topic model does not produce meaningful results everytime. Our next task is to fine tune the parameters, such as the number of topics expected, to determine what number of topics produce the most coherent model.

2. Additionally, we will include the time factor into our corpus, in order to see how the topics change over time. Currently, our corpus is not split by time.